{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning With Probability and Pyro â€” Is My Model Good Enough?\n",
    "- [talk](https://www.youtube.com/watch?v=5f-9xCuyZh4)\n",
    "- [article](https://towardsdatascience.com/reasoning-with-probability-is-my-model-good-enough-1cfd27d5aed9)\n",
    "- [Pyro Tutorials](http://pyro.ai/examples/#introductory-tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyro \n",
    "import torch \n",
    "import pyro\n",
    "pyro.set_rng_seed(102563)\n",
    "\n",
    "import pyro.distributions as dist \n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from pyro.infer.mcmc import HMC, MCMC\n",
    "\n",
    "assert pyro.__version__.startswith('1.5.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_X, MNIST_Y  = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "print(MNIST_X.shape)\n",
    "print(MNIST_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MNIST_X, MNIST_Y, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = DecisionTreeClassifier(min_samples_leaf=2)\n",
    "my_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = my_tree.score(X_test, y_test)\n",
    "print(f'Accuracy is {\"{0:.2f}\".format(accuracy*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the luxury of having 14,000 labeled examples in the test set but what does this look like if we were to only have 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_100_test, _, y_100_test = train_test_split(X_test, y_test, test_size=(100/X_test.shape[0]), random_state=1)\n",
    "print(X_100_test.shape, y_100_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = my_tree.score(X_100_test, y_100_test)\n",
    "print(f'Accuracy is {\"{0:.2f}\".format(accuracy*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having faith in a model is inherently dependent on the number of samples that we are able to test it with.   \n",
    "  \n",
    "Our confidence in the test's set score increases as the size of the test set increases.  \n",
    "  \n",
    "With [pyro](http://docs.pyro.ai/en/1.1.0/index.html) we can numerically estimate our confidence and how much room for error we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyro\n",
    "1. **Model Function:** \n",
    "    - simulates our underlying model (the process that results in correct or incorrect labels) which starts as our original, or prior, distribution\n",
    "2. **Kernel:** \n",
    "    - measures the likelihood of the observed examples. This happens as the model function produces them. \n",
    "3. **Sampler:**\n",
    "    - builds the updated, or posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define our observations and generate an binary array that indicates if our predictions were correct or not.   \n",
    "  \n",
    "We then convert these to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our observations\n",
    "y_100_pred = my_tree.predict(X_100_test)\n",
    "correctness_values_100 = y_100_pred == y_100_test   # An array of   \n",
    "                                                    # observations \n",
    "                                                    # that shows\n",
    "                                                    # if our \n",
    "                                                    # predictions \n",
    "                                                    # match the test\n",
    "                                                    # or not\n",
    "\n",
    "y_pred = my_tree.predict(X_test)\n",
    "correctness_values = y_pred == y_test\n",
    "\n",
    "correctness_values_100 = torch.from_numpy(correctness_values_100).float()\n",
    "correctness_values = torch.from_numpy(correctness_values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the **Model Function**.     \n",
    "- It will accept our observations and try to sample from a prior distribution and calculate the likelihood of the prior based on our observations.   \n",
    "- We use a uniform distribution here but any [torch distribution](https://pytorch.org/docs/master/distributions.html) is available depending on our knowledge of the underlying generating process of a value \n",
    "  \n",
    "And the **Kernel** \n",
    "- It will then update the prior to a more likely posterior distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(y):\n",
    "    # Our observations are binary observations which come\n",
    "    # from a Bernoulli distribution with some percent\n",
    "    # \"p\" to be correct and (1-p) of being wrong\n",
    "    \n",
    "    # we want to estimate that p value. We start not\n",
    "    # knowing anything about it, so our prior will be\n",
    "    # a uniform distribution from 0.0 to 1.0\n",
    "    underlying_p = pyro.sample(\"p\", dist.Uniform(0.0, 1.0)) \n",
    "\n",
    "    # for each observation\n",
    "    for i in range(len(y)):\n",
    "\n",
    "        # our hidden distribution\n",
    "        y_hidden_dist = dist.Bernoulli(underlying_p)\n",
    "\n",
    "        # now sample from our distribution conditioned on our \n",
    "        # observation\n",
    "        y_real = pyro.sample(\"obs_{}\".format(i), \n",
    "                             y_hidden_dist, \n",
    "                             obs = y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.sample??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First clear the old values of all our stored parameters\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# the kernel we will use\n",
    "hmc_kernel = HMC(model,\n",
    "                 step_size = 0.1)\n",
    "\n",
    "\n",
    "# the sampler which will run the kernel\n",
    "mcmc = MCMC(hmc_kernel, num_samples=100, warmup_steps=100)\n",
    "\n",
    "# the .run method accepts as parameter the same parameters our model function uses\n",
    "mcmc.run(correctness_values_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = mcmc.get_samples(num_samples=5000)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(sample_dict['p'].numpy(), color=\"orange\");\n",
    "plt.xlabel(\"Observed probability value\")\n",
    "plt.ylabel(\"Observed frequency\")\n",
    "plt.show();\n",
    "\n",
    "mcmc.summary(prob=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
